{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lLpvxA-ijqCV",
        "outputId": "d58e86be-6a64-484b-922e-271232617d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOmf0N29QRVJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"CohleM/english-to-nepali\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bkS9ONW1jvG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english=dataset['train']['en'][:20000]\n",
        "nepali=dataset['train']['ne'][:20000]"
      ],
      "metadata": {
        "id": "V7Op-0mPjzMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "      self.vocab={}\n",
        "\n",
        "    def load(self,path):\n",
        "      with open(path, \"rb\") as file:\n",
        "        self.vocab=pickle.load(file)\n",
        "\n",
        "    def train(self,text, vocab_size=1000):\n",
        "        tokens = text.encode(\"utf-8\")\n",
        "        vocab_size = vocab_size\n",
        "        num_merges = vocab_size - 256\n",
        "        merges = {}\n",
        "        ids = list(tokens)\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            stats = self.get_stats(ids)\n",
        "            pair = max(stats, key=stats.get)\n",
        "            idx = 256 + i\n",
        "            ids = self.merge(ids, pair, idx)\n",
        "            merges[pair] = idx\n",
        "\n",
        "        self.vocab['vocab'] = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in merges.items():\n",
        "            self.vocab['vocab'][idx] = self.vocab['vocab'][p0] + self.vocab['vocab'][p1]\n",
        "\n",
        "        self.vocab['merges']=merges\n",
        "\n",
        "        return self.vocab\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "        counts = {}\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        new_ids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "                new_ids.append(idx)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_ids.append(ids[i])\n",
        "                i += 1\n",
        "        return new_ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        token = b\"\".join(self.vocab['vocab'][idx] for idx in ids)\n",
        "        text = token.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    def encode(self, text):\n",
        "        token = list(text.encode(\"utf-8\"))\n",
        "        while len(token) >= 2:\n",
        "            stats = self.get_stats(token)\n",
        "            pair = min(stats, key=lambda p: self.vocab['merges'].get(p, float(\"inf\")))\n",
        "            if pair not in self.vocab['merges']:\n",
        "                break\n",
        "            idx = self.vocab['merges'][pair]\n",
        "            token = self.merge(token, pair, idx)\n",
        "        return token\n"
      ],
      "metadata": {
        "id": "_VclmW3yj14L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NVQP8a2ETL2",
        "outputId": "103b977a-cb19-4e0e-9e28-24ede6fd1038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# en_tokenizer=Tokenizer()\n",
        "# en_tokenizer.load(\"/content/drive/MyDrive/English_Nepali/English_Tokenizer_500.pkl\")"
      ],
      "metadata": {
        "id": "_Gfdg1rXj3MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np_tokenizer=Tokenizer()\n",
        "# np_tokenizer.load(\"/content/drive/MyDrive/English_Nepali/Nepali_Tokenizer_500.pkl\")"
      ],
      "metadata": {
        "id": "LPfZLwUOkBL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en_tokenizer.vocab['vocab'][1002] = b'<sos>'\n",
        "# en_tokenizer.vocab['vocab'][1001]=b'<eos>'\n",
        "# en_tokenizer.vocab['vocab'][1000] = b'<pad>'\n",
        "\n",
        "# np_tokenizer.vocab['vocab'][1002] = b'<sos>'\n",
        "# np_tokenizer.vocab['vocab'][1001]=b'<eos>'\n",
        "# np_tokenizer.vocab['vocab'][1000] = b'<pad>'"
      ],
      "metadata": {
        "id": "mqTcklKjkCpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.load(\"/content/drive/MyDrive/English_Nepali/English_Nepali\")"
      ],
      "metadata": {
        "id": "HdWfS0KDkSb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab['vocab'][1002] = b'<sos>'\n",
        "tokenizer.vocab['vocab'][1001]=b'<eos>'\n",
        "tokenizer.vocab['vocab'][1000] = b'<pad>'"
      ],
      "metadata": {
        "id": "QHuCfMUGkULv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset=torch.load('/content/drive/MyDrive/English_Nepali/dataset.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mtam7nFckEze",
        "outputId": "67712ecc-548e-4b91-dbf5-099d9942ac31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-f526ebeeecd2>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset=torch.load('/content/drive/MyDrive/English_Nepali/dataset.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=torch.load('/content/drive/MyDrive/English_Nepali/one_token_dataset.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1gfkBIrkktX",
        "outputId": "07d807a5-347c-4086-b532-0909d85c4daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-5a8ea982a971>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset=torch.load('/content/drive/MyDrive/English_Nepali/one_token_dataset.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    english_batch, nepali_batch = zip(*batch)\n",
        "\n",
        "    max_len_english = max(len(seq) for seq in english_batch)\n",
        "    max_len_nepali = max(len(seq) for seq in nepali_batch)\n",
        "\n",
        "    padded_english_batch = pad_sequence(english_batch, batch_first=True, padding_value=1000)\n",
        "    padded_nepali_batch = pad_sequence(nepali_batch, batch_first=True, padding_value=1000)\n",
        "\n",
        "    padded_english_batch = F.pad(padded_english_batch, (0, max_len_english - padded_english_batch.size(1)))\n",
        "    padded_nepali_batch = F.pad(padded_nepali_batch, (0, max_len_nepali - padded_nepali_batch.size(1)))\n",
        "\n",
        "    return padded_english_batch, padded_nepali_batch\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, drop_last=True)"
      ],
      "metadata": {
        "id": "sh2k9WKVkGzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=next(iter(dataloader))\n",
        "data[0].shape,data[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7xbY350kO38",
        "outputId": "2f914cb9-0e72-471a-fbaa-f3690f98d73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 172]), torch.Size([32, 161]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,dropout_p=0.1):\n",
        "    super().__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    self.embedding=nn.Embedding(num_embeddings=input_size,embedding_dim=hidden_size)\n",
        "    self.RNN=nn.GRU(input_size=hidden_size,hidden_size=hidden_size,batch_first=True)\n",
        "    self.dropout=nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.dropout(self.embedding(x))\n",
        "      out,hidden=self.RNN(x)\n",
        "\n",
        "      return out,hidden"
      ],
      "metadata": {
        "id": "88qAuxHzG1-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,hidden_size):\n",
        "    super().__init__()\n",
        "    self.Wa=nn.Linear(hidden_size,hidden_size)\n",
        "    self.Ua=nn.Linear(hidden_size,hidden_size)\n",
        "    self.Va=nn.Linear(hidden_size,1)\n",
        "\n",
        "  def forward(self,query,keys):\n",
        "    scores=self.Va(torch.tanh(self.Wa(query)+self.Ua(keys)))\n",
        "    scores=scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "    weights=F.softmax(scores,dim=-1)\n",
        "    context=torch.bmm(weights,keys)\n",
        "\n",
        "    return context,weights"
      ],
      "metadata": {
        "id": "qM_D_ppNHCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size,dropout_p=0.1):\n",
        "    super().__init__()\n",
        "    self.embedding=nn.Embedding(output_size,hidden_size)\n",
        "    self.attention=Attention(hidden_size)\n",
        "    self.RNN=nn.GRU(2*hidden_size,hidden_size,batch_first=True)\n",
        "    self.out=nn.Linear(hidden_size,output_size)\n",
        "    self.dropout=nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
        "    MAX_LENGTH=target_tensor.shape[1] if target_tensor is not None else 20\n",
        "    batch_size=encoder_outputs.size(0)\n",
        "    decoder_input=torch.empty(batch_size,1,dtype=torch.long,device=device).fill_(1002)\n",
        "    decoder_hidden=encoder_hidden\n",
        "    decoder_outputs=[]\n",
        "    attentions=[]\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "      decoder_output,decoder_hidden,attn_weights=self.forward_step(decoder_input,decoder_hidden,encoder_outputs)\n",
        "      decoder_outputs.append(decoder_output)\n",
        "      attentions.append(attn_weights)\n",
        "\n",
        "      if target_tensor is not None:\n",
        "        decoder_input=target_tensor[:,i].unsqueeze(1)\n",
        "      else:\n",
        "        _,topi=decoder_output.topk(1)\n",
        "        decoder_input=topi.squeeze(-1).detach()\n",
        "\n",
        "    decoder_outputs=torch.cat(decoder_outputs,dim=1)\n",
        "    decoder_outputs=F.log_softmax(decoder_outputs,dim=-1)\n",
        "    attentions=torch.cat(attentions,dim=1)\n",
        "\n",
        "    return decoder_outputs,decoder_hidden,attentions\n",
        "\n",
        "  def forward_step(self,input,hidden,encoder_outputs):\n",
        "    embedded=self.dropout(self.embedding(input))\n",
        "    query=hidden.permute(1,0,2)\n",
        "    context,attn_weights=self.attention(query,encoder_outputs)\n",
        "    input_rnn=torch.cat((embedded,context),dim=2)\n",
        "\n",
        "    output,hidden=self.RNN(input_rnn,hidden)\n",
        "    output=self.out(output)\n",
        "\n",
        "    return output,hidden,attn_weights\n"
      ],
      "metadata": {
        "id": "GwZ18FC-OFmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "QY8bQt8MmXu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab['vocab'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlLvJSfNmX13",
        "outputId": "df9f46e3-b12f-4b4d-d490-ae2104c4e866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "\n",
        "encoder = EncoderRNN(len(tokenizer.vocab['vocab']), hidden_size).to(device)\n",
        "decoder=AttnDecoderRNN(hidden_size, len(tokenizer.vocab['vocab'])).to(device)\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "xrx5FOb8Pcqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss=[]\n",
        "\n",
        "for i in range(15):\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  batch_train_loss=[]\n",
        "  for batch in tqdm(dataloader):\n",
        "    x_train=batch[0].to(device)\n",
        "    y_train=batch[1].to(device)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(x_train)\n",
        "    decoder_outputs,decoder_hidden, _,= decoder(encoder_outputs,encoder_hidden, y_train)\n",
        "    loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)),y_train.view(-1))\n",
        "    batch_train_loss.append(loss.item())\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "  train_loss.append(sum(batch_train_loss)/len(batch_train_loss))\n",
        "  print(f\"Epoch={i}\\tTrain Loss={sum(batch_train_loss)/len(batch_train_loss)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-4i3RLNmY1C",
        "outputId": "e9142a57-d6ac-44f9-e89e-235598360af5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:00<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0\tTrain Loss=1.3677364879131317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 57/625 [00:22<03:53,  2.43it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "eng_text=english[0]\n",
        "target=nepali[0]\n",
        "with torch.no_grad():\n",
        "  input_tensor=torch.tensor(tokenizer.encode(eng_text)+[1001]).unsqueeze(0).to(device)\n",
        "  encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "  decoder_outputs,decoder_hidden,decoder_attn= decoder(encoder_outputs,encoder_hidden)\n",
        "  _, topi = decoder_outputs.topk(1)\n",
        "  decoded_ids = topi.squeeze()\n",
        "\n",
        "  decoded_words = []\n",
        "  for idx in decoded_ids:\n",
        "    if idx.item() == 1001:\n",
        "      decoded_words.append(1001)\n",
        "      break\n",
        "    decoded_words.append(idx.item())\n",
        "print(f\"input:{eng_text}\")\n",
        "print(f\"target:{target}\")\n",
        "print(f\"predict:{tokenizer.decode(decoded_words)}\")"
      ],
      "metadata": {
        "id": "ksq1v-aBngqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f1d6b2-e049-4f0c-bfdd-9025c13762b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:Technical committees will be attached to each ministry.\n",
            "target:प्रत्येक मन्त्रालय अन्तर्गत शिल्प (टेक्निकल) कमिटीहरु गठन गरिनेछन्\n",
            "predict:तपाईंले तपाईंका सेवकहरू सुलेमानले आफ्ना सुलेमानले आफ्न\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CeV7NOFnyQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}